<html>
  <body>
    <h2>Survey on the Use of Classification Graphs in the Design of Energy-Efficient Memristor Crossbar Circuits</h2>
    <p>Memristors are two-terminal electronic devices whose resistance is affected by one or more internal state variables. Ohm's rule, which depends on the state, describes a memristor. Its resistance is determined by the applied voltage or current across the memristor's complete past signal waveform. Memristors have a lot of practical applications because they allow for the establishment of circuit features that resistors, capacitors and inductors cannot. Spintronic devices, ultra-dense information storage, neuromorphic circuits, and programmable electronics are examples of memristors' potential novel uses. Modern decision tree methods employ heuristics recursively to construct each split separately, which might not accurately capture the dataset's underlying characteristics. By building the complete decision tree at once to achieve global optimality, the optimal decision tree problem aims to solve this. In order to apply decision trees using flow-based computing, Pranav Sinha and Sunny Raj [1] propose an approach for designing in-memory, energy-efficient, and compact memristor crossbar circuits. They create a brand-new method called a binary classification graph, which is equally accurate to decision trees but makes choices based on bit values of input features rather than thresholds. Due to the use of sneak paths in computations, their suggested design is robust to manufacturing errors and can grow to large crossbar sizes. On various machine learning datasets, they train decision tree models, and then they produce crossbar designs that correlate to those models. They use SPICE models to check the accuracy of our designs and see how pre-existing hardware flaws affect the size of the memristor crossbars. They conclude by comparing the energy usage of our designs to those of other techniques, demonstrating that our approach outperforms cutting-edge designs in terms of energy efficiency and circuit size.<br>
      Designing circuits for implementing random forest by combining numerous crossbar arrays would be a straightforward extension of the work which I believe can be a scope of implementation here. Designing circuits for more intricate tree and graph neural networks and incorporating flow-based computing's robustness and low-energy utilization into neural networks would be a more ambitious course of action.</p><b>
      <h3>Bibliography:</h3>
      <p>When compared to other machine learning techniques, decision trees have a significant edge in terms of explanability and can perform well on a variety of datasets. For a variety of machine learning activities, including accelerating decision trees (DT) and neuromorphic computing, in-memory computing has been used. The proposed memristor-based LSTM circuit system was introduced using the extensively used for sentiment analysis IMDB dataset and SemEval dataset [2]. An appealing option for extreme power efficiency is in-memory computing and DL accelerators built on memristors. The implementation of power-efficient in-memory computing, DL accelerators, and spiking neural networks could be achieved with the help of memristors, a new beyond-complementary metal-oxide-semiconductor (CMOS) technology that is reviewed in this article [3]. A novel analog content addressable memory (CAM) based on developing memristor devices was suggested by Pedretti et al. [4] for quick look-up table operations. Here, they suggest using the analog CAM as an in-memory algorithmic primitive to accelerate tree-based model inference. The fundamental energy efficiency constraints of digital logic are modified by memristor crossbar circuits, which can execute analog matrix-vector multiplications. For a select few neural network applications, they have been demonstrated to perform well in specialized accelerators. Ankit et al. [6] introduce the Programmable Ultra-efficient Memristor Based Accelerator (PUMA), which improves memristor crossbars with general purpose execution units to allow the acceleration of a broad variety of Machine Learning (ML) inference workloads.</p>
      
      <h4>Reference:</h4>
      
  </body>
</html>
